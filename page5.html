<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v4.12.4, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v4.12.4, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/logo2.png" type="image/x-icon">
  <meta name="description" content="Web Page Generator Description">
  
  
  <title>Report</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/dropdown/css/style.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="preload" as="style" href="assets/mobirise/css/mbr-additional.css"><link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
</head>
<body>
  <section class="menu cid-qTkzRZLJNu" once="menu" id="menu1-4">



    <nav class="navbar navbar-expand beta-menu navbar-dropdown align-items-center navbar-fixed-top navbar-toggleable-sm">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
                <span></span>
            </div>
        </button>
        <div class="menu-logo">
            <div class="navbar-brand">

                <span class="navbar-caption-wrap"><a class="navbar-caption text-white display-4" href="index.html">
                        CRAP-E</a></span>
            </div>
        </div>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav nav-dropdown" data-app-modern-menu="true"><li class="nav-item">
                    <a class="nav-link link text-white display-4" href="https://mobirise.com">
                        </a>
                </li><li class="nav-item"><a class="nav-link link text-white display-4" href="https://mobirise.com">
                        </a></li><li class="nav-item">
                <a class="nav-link link text-white display-4" href="index.html" aria-expanded="false"><span class="mbri-home mbr-iconfont mbr-iconfont-btn"></span></a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="page5.html" aria-expanded="false"><span class="mbri-file mbr-iconfont mbr-iconfont-btn"></span>Report</a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="page4.html" aria-expanded="false"><span class="mbri-rocket mbr-iconfont mbr-iconfont-btn"></span>Media</a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="page2.html"><span class="mbri-user mbr-iconfont mbr-iconfont-btn"></span>Our Team</a></li><li class="nav-item"><a class="nav-link link text-white display-4" href="https://mobirise.com">
                        </a></li></ul>
            <div class="navbar-buttons mbr-section-btn"><a class="btn btn-sm btn-primary display-4" href="https://www.youtube.com/watch?v=XUZmgWbbwdM"><span class="mbri-video mbr-iconfont mbr-iconfont-btn"></span>

                    What is CRAP-E?</a></div>
        </div>
    </nav>
</section>

<section class="engine"><a href="https://mobirise.info/m">site design templates</a></section><section class="header1 cid-s2EyoU2OBi mbr-parallax-background" id="header1-x">

    

    <div class="mbr-overlay" style="opacity: 0.6; background-color: rgb(118, 118, 118);">
    </div>

    <div class="container">
        <div class="row justify-content-md-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center mbr-bold pb-3 mbr-fonts-style display-5">Optimisation of a small scale planetary rover </h1>
                <p class="mbr-text align-center pb-3 mbr-fonts-style display-7">
                    Controllable Rover Automation Project - Europe [CRAP-E]
                </p>
                <p class="mbr-text align-center pb-3 mbr-fonts-style display-7">
                    Jonas Tjepkema, Marco Fiorito, Francesco Pelizza, Timon Schapals, Juliette Passariello, Adjaye Adams
                </p>
                <div class="mbr-section-btn align-center"><a class="btn btn-md btn-info display-4" href="page5.html#content9-9">Intro</a> <a class="btn btn-md btn-info display-4" href="page5.html#content9-b">Materials &amp; Methods</a> <a class="btn btn-md btn-info display-4" href="page5.html#content9-d">Results and Discussion
                        </a> <a class="btn btn-md btn-info display-4" href="page1.html#content21-9">
                        Conclusion</a> <a class="btn btn-md btn-info display-4" href="page1.html#content1-1n">
                        References</a></div>
            </div>
        </div>
    </div>

</section>

<section class="mbr-section article content9 cid-s2DGMDPdzu" id="content9-9">
    
     

    <div class="container">
        <div class="inner-container" style="width: 100%;">
            <hr class="line" style="width: 25%;">
            <div class="section-text align-center mbr-fonts-style display-5">
                    Introduction</div>
            <hr class="line" style="width: 25%;">
        </div>
        </div>
</section>

<section class="mbr-section article content1 cid-s2EJIyMPIp" id="content1-y">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>This research project is the continuation
                of a previously completed Mars rover construction (Fonau, Tjepkema, Fiorito, et al. 2020) which
                constructed a programmable robot meant to be deployed and explore an environment in conditions similar
                to the planet Mars. We designed the rover to: investigate an extraplanetary environment, look for ancient
                forms of life and organic compounds, and collect data on the physical properties of a Mars-like
                environment. The main objective of this project was to improve and optimise the rover to increase its
                capabilities. The original design of CRAP-E was based on its predecessor rover, the Brover, which itself
                was constructed following the design of Swappy the Rover (version 1.0); a model of the Perseverance and
                Curiosity Mars rover models. There are several advances that distinguish CRAP-E from its predecessor,
                the Brover. The drivetrain of the Rover was redone, it was equipped with a functional robotic arm, a
                myriad of sensors and orientation devices were added, alongside a solar panel.
            </p><p><strong><br></strong></p></div>
        </div>
    </div>

    <div class="container">
        <div class="media-container-row">
             <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>To reach the data collection objective of
                this project, CRAP-E was equipped with a series of sensors, cameras and a robotic arm. Concerning the
                sensors, the rover was equipped with the following modules: a temperature and humidity sensor (DHT22),
                an FC-28 soil humidity detector, a Positioning System (GPS) receiver and a temperature and pressure sensor
                (BME280). To reach the automation objective the rover was equipped with a LiDAR, five ultrasound sensors
                with the intention of artificial intelligence (AI) taking their inputs and moving the rover based on
                that. In regards to communication and sending of data to external computers, the rover was equipped
                with a radio frequency transceiver (nRF24L01).
            </p><p><strong><br></strong></p></div>
        </div>
    </div>

    <div class="container">
        <div class="media-container-row">
             <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>Fundamentally, the objective of this
                project was to improve a previously built rover, through the addition of extra sensor modules, an
                autonomous driving system, a functional robotic arm and re-designed parts. More specifically, the
                project was to be considered successful with a functioning rover that could autonomously move from one
                location to another, carry out a series of scientific functions, take pictures and provide all the
                collected data through a remote communication system, either wi-fi or radio signal.
            </p><p><strong><br></strong></p></div>
        </div>
    </div>
</section>

<section class="mbr-section article content9 cid-s2DGTYTwzg" id="content9-b">
    
     

    <div class="container">
        <div class="inner-container" style="width: 100%;">
            <hr class="line" style="width: 25%;">
            <div class="section-text align-center mbr-fonts-style display-5">
                    Materials &amp; Methods</div>
            <hr class="line" style="width: 25%;">
        </div>
        </div>
</section>

<section class="mbr-section article content1 cid-s2DGW4e0iE" id="content1-c">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>

            </p><p><br></p><p><strong>Hardware&nbsp;</strong></p><p><strong><br></strong></p>

                <p><strong>Sensor Mast Design</strong></p>

                <p>TA sensor mast was designed to provide a sufficient vantage point for the cameras and sensors. The
                    mast was designed to house a wide angle infrared camera, a thermal sensor, the LiDAR, and a slot for
                    an optional normal camera. To allow for greater operational freedom, the sensor mast was designed
                    to allow for pan and tilt movement of the cameras, which would not only allow them to be used in
                    capturing larger panoramics and stitch images together, but also allow the sensors to be used as
                    diagnostic tool capable of surveying a large portion of the rover body, and documenting the
                    condition of parts. However, the LiDAR could not be mounted onto the same rotating base plate as
                    the other sensors, as this would introduce degrees of freedom that would then have to be taken into
                    account when processing the incoming data. To remove this unnecessary complication, the sensor mast
                    was designed with a stable top-plate onto which the LiDAR could be mounted. The support for this
                    top plate was designed to be as unobtrusive as possible to the field of view of the moving sensors
                    below.
                </p>
            </div>
        </div>
    </div>
</section>

<section class="cid-s3o5R9nIvc" id="image3-1e">

    

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBsensordrawing.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

<section class="mbr-section article content1 cid-s3o677EF4P" id="content1-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
                <strong> </strong></p><p><em>Figure 1: Sensor Mast final design</em></p>

                <p><strong>Thermal and Infrared Cameras</strong></p><p><span style="font-size: 1rem;">

                     Instead of just a normal optical camera, it was decided that an infrared camera with dual infrared
                    leds to illuminate the scene would allow for greater operational capabilities. Through the use of
                    this infrared camera, and the sensor's ability to provide its own light source, the rover is capable
                    of day/night operation and also in environments with poor lighting conditions. There is also a
                    thermal sensor mounted to the same housing. This thermal sensor has a resolution of 8 x 8, as seen
                    in figure H, which can be increased to 64 x 64 through the use of interpolation (bicubic in our case).
                    The thermal and infrared sensors are fixed onto the saem housing to ensure they are viewing the
                    same things, which allows for their data to be superimposed onto one another, allowing for
                    simultaneous analysis of thermal and visual data. Figure H shows a heat point where the human body
                    stands

                </span></p><p><span style="font-size: 1rem;"><br></span></p>
            </div>

        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image4-1e">



    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBthermalscene.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

<section class="mbr-section article content1 cid-s3o677EF4P" id="content2-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
                <strong> </strong></p><p><em>Figure 2: Thermal camera and normal camera image of the same scene</em></p>

                <p><strong>LiDAR Operation</strong></p><p><span style="font-size: 1rem;">

                     The LiDAR used on the rover was an X4 produced by YDLiDAR, with a 360-degree, omnidirectional
                    scanning and 5 kilohertz range finder frequency. It can cover a 2D plane within a 10 metre range
                    of the LiDAR, and the incoming data can be processed into a point cloud to model the surroundings
                    (YDLiDAR User Manual). Although such devices are designed to survey a 2D plane, they can be fixed
                    to rotating mounts, allowing them to survey in 3D. However such a modification requires precise
                    readings for the inclination angle, and given its complexity, would require more time and technical
                    knowledge than the project timeframe allowed. Instead it was decided that the extent of the LiDAR
                    would be to create instantaneous point clouds of the surroundings, and if possible, perform simple
                    SLAM.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Solar Panel Mount</strong></p><p><span style="font-size: 1rem;">

                     The rover was designed to have a small solar panel mounted on the front half of the body.
                    Originally, the solar panel was designed to rotate around one end, while the other side is free to
                    be moved up and down. This allows it to face a range of angles from directly up to horizontal.
                    This, combined with the panning capabilities of the rover itself, would theoretically grant the
                    solar panel two degrees of freedom, and therefore maximise its efficiency when charging. Given the
                    size of the solar panel and the free space available on the top of the rover, it was decided to
                    elevate the panel by some 6 cm in order for it to clear the rocker and bogie suspension setup. This
                    would also allow more space within the front of the rover body for future component designs.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Sample Collection Box</strong></p><p><span style="font-size: 1rem;">

                    To accommodate the new rover arm, a sample collection box was designed in order to hold samples
                    which could then be analysed with a X-ray fluorescence spectrometer. The design utilized an
                    automatic door moved via a servo motor attached to the side of the box in order to open and close
                    the box when the sample needed to be emptied.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Robotic Arm/Grabber</strong></p><p><span style="font-size: 1rem;">

                    The goal of the robotic arm was to allow the rover to pick up materials for further analysis within
                    the rover. The previous version of the arm, featuring a bucket, was scrapped, as it failed to work
                    due to excessively long and heavy links of the arm, that put too much strain on the servos.
                    Therefore, special consideration was given to the problems and limitations of the previous design.
                    The arm was placed as low as possible on the rover, without compromising ground clearance. The arm
                    was designed in Fusion 360, with the exception of the grabber, which was taken from a design by
                    “How to mechatronics” (2018). The chosen tool is a grabber. It allows the arm to pick up objects
                    with high precision. There are 6 servos in total that operate the arm. Three of them are  MG996R
                    servos by DIY MORE, which can rotate 270 degrees, weigh 55 g and have 1.5 Nm of torque at 6 volts.
                    They are used to rotate the whole arm and move the first and second link of the arm. The first link
                    and the rotation base each feature a hook. A rubber band can be spanned in between which exerts
                    about 0.1 Nm of torque on the arm when it is positioned horizontally. This helps the motor to lift
                    a heavier arm. A different number of rubber bands can be applied for different levels of support for
                    the motor.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p>The three smaller motors are SG90 Micro servos sourced from amazon. At the end of the rover, there was no need for high torque servos, and weight is much more critical.  With 9 g of weight and 0.16 Nm of torque, these motors were chosen for the movement of the gripper. They change the roll and pitch of the gripper as well as open and close it. The five degrees of freedom for the movement allow the arm to reach objects in a whole area and find the optimal orientation and angle of attack for the gripper. The rover arm is long enough to access a circular area on the ground with a 50 cm diameter. This is a good compromise for the weight of the arm and its action radius. The rotation of the arm also allows it to move the grabber to the inside of the rover where the grabber can release material to a collection box for analysis. The control of the rover happens through the Raspberry Pi, where each servo is connected to a GPiO pin. Through the Web App, each joint can be moved by a slider to operate the arm.

            </p><p><strong><br></strong></p>

            </div>

        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image5-1e">



    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBroboticarm.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content3-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
                <strong> </strong></p><p><em>Figure 3: Components of robotic arm</em></p>

                <p><strong>PETG Replacement Parts</strong></p><p><span style="font-size: 1rem;">

                     Parts printed out of polylactic acid (PLA) that broke or cracked were replaced with reinforced pieces made of Glycol modified Polyethylene Terephthalate (PET-G). Multiple parts made of PLA that were under high stress in the drivetrain and suspension assembly failed. Pieces were especially prone to failing where they had press fit bearings. Several factors were suspected of compromising the strength of the parts which were addressed with the replacement parts. Time constraints in the last project to print the whole rover in time for the end of the project led to time being prioritized over part strength. Pieces were printed with thin walls of 1.35mm thickness and infill that was more optimized for print time than for strength.


                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p>PET-G was chosen as a new material as it is less brittle than PLA (Vinyas 2019). The hypothesis was that parts would be able to deform without part failure; a desirable property with press fit bearings. The dimensional accuracy of 3D printers can vary. By calibrating the 3D printer through test prints and subsequent changes in the software, the dimensional accuracy was increased. This meant that press fit parts only had to flex as much as intended and weren't put under higher stress than intended.
The print settings were changed to give a stronger part. The wall thickness for most new parts was increased to 3.2mm and the infill pattern was changed to gyroid, which features isotropic response to loading.

            </p><p><strong><br></strong></p>

                <p>
                The new pieces were strategically reinforced in areas that failed previously. The software used to turn a 3D model into commands for the printer of where to let our plastic, called a slicer, was changed for the replacement parts. The new Software, PrusaSlicer 2.3, allows local changes of  printing parameters of the object. This was used to make the area around critical points stronger, allowing the parts to be strong where needed and be light elsewhere, alongside shorter printing times, and less material use.
                 </p><p><strong><br></strong></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image6-1e">



    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBB3dinfill.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content4-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
                <strong> </strong></p><p><em>Figure 4: Suspension part with local reinforcement around bearing</em></p>

                 <p>
                The figure above shows an example of local reinforcement. In most of the part, the red gyroid infill can be seen in the interior of the piece. Around the bearing, a thick cylinder of reinforcement can be seen to strengthen the part.
                 </p><p><strong><br></strong></p>

                <p><strong>Sensors</strong></p><p><span style="font-size: 1rem;">

                     The rover model optimization involves the addition of several sensors capable of acquiring data from the environment. Seven types of sensors were added to obtain atmospheric, terrestrial, and locational data, used to generate a complete picture of the area that the rover explores.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>


                <p>MQ2 – Gas Sensor</p>
                <p>
The MQ2 is a Metal Oxide Semiconductor (MOS) gas sensor, sensitive to the presence of reducing gases such as Liquefied Petroleum Gas (LPG), Alcohol, Propane, Hydrogen, Methane, and Carbon Monoxide. The resistance of the material changes when in contact with the gas allowing the chemiresistor to detect gas concentrations ranging from 200ppm to 1000ppm. The output voltage is proportional to the concentration of gas. Thus, the module not only provides a binary value as a response to detecting the presence of gas, but it also returns an analog interpretation of its concentration in the atmosphere (Last Minute Engineer, n.d.).
            </p><p><strong><br></strong></p>

                <p>HC-SR04 – Ultrasonic Sensor</p>
                <p>
        	The HC-SR04 is a sensor that measures distance through the collection of ultrasonic data. The module produces an ultrasound wave at 40kHz; this wave travels through air and, in the case where an object is nearby, the wave bounces back from the object to the module. Subsequently, the time of travel from the wave is collected and, taking into consideration the speed of the ultrasonic wave, the distance from possible obstacles can be calculated (Jabbar, 2019).
            </p><p><strong><br></strong></p>

                 <p>DHT11 - Temperature & Humidity Sensor</p>
                <p>
The DHT11 is a module that records values of humidity and temperature through a sensing component and a Thermistor. The humidity sensing part of the module contains a moisture holding substrate in between two electrodes; the substrate releases ions when vapor water is present and thus, the conductivity is increased proportionally. Additionally, it contains a thermal resistor as a second component; in this case, the resistance is inversely proportional to the temperature detected. These two pieces allow the sensor to measure temperature from 0-59ºC and humidity from 20-80% (Last Minute Engineers, n.d.)
            </p><p><strong><br></strong></p>

                 <p>BMP280 – Pressure & Altitude</p>
                <p>
	The BMP280 is an environmental sensor capable of acquiring data for several variables, including temperature, humidity, and barometric pressure. This sensor is capable of detecting pressure from 300Pa to 1100hPa, and these values are taken with a ±1hPa absolute accuracy. Given the high precision that the sensor provides for pressure measurements, it can also be used as an altimeter to collect data for relative altitude, with a ±1m accuracy (Adafruit, n.d.).
            </p><p><strong><br></strong></p>

                <p>MH – Soil Moisture</p>
                <p>
	The MH is a sensor that measures the volume of water in the soil. The module contains two probes that transmit electric current through the soil. When there is water present in the soil and the moisture levels are high, the resistance to the electric current is reduced. Conversely, dry soil reduces conductivity and thus resistance between the probes is higher (Arduino, 2019).
            </p><p><strong><br></strong></p>

                 <p>GPS6MV2 – Global Positioning System</p>
                <p>
	The GPS module is a navigation system using satellite information. The module receives a satellite’s unique signal and their orbital parameters from which, after decoding and taking into account the time required to receive the signal, it is possible to obtain the precise distance and location of the satellite. Connecting to three satellites allows to triangulate the exact location of the user (their latitude and longitude). Additionally, if the GPS were connected to four satellites, a 3D position of the user can be determined (latitude, longitude, and altitude). This module serves as a proof of concept for location tracking in the rover (Arduino, 2019).
            </p><p><strong><br></strong></p>

                <p>NRF24l01 – Wireless modules</p>
                <p>
	The NRF24l01 transceivers are a wireless interface that allows data transfer through radio frequency. These modules send data, remotely monitor it and control specific features from a distance. The transceivers are designed to function with 2.4GHz, a band reserved internationally for the use of unlicensed low-powered devices, using Gaussian Frequency-Shift Keying (GFSK) for the transmission of data (Last Minute Engineers, 2020).
            </p><p><strong><br></strong></p>

                <p><strong>Sensor Box Design</strong></p><p><span style="font-size: 1rem;">

                     A holding box was exclusively designed to integrate the sensors to the rover. The box has individual places for each sensor, and was designed with spaces made to minimize the wire length required to connect the sensors to the Arduino microcontroller; these connections go directly to the pins of the modules

                </span></p><p><span style="font-size: 1rem;"><br></span></p>


            </div>
        </div>
    </div>
</section>

    <section class="cid-s3o5R9nIvc" id="image7-1e">



    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBsensorbox.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content5-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
                <strong> </strong></p><p><em>Figure 5: Sensor box housing for various sensors</em></p>

                </p><p><br></p><p><strong>Software&nbsp;</strong></p><p><strong><br></strong></p>

                <p><strong>Obstacle avoidance through Machine Learning</strong></p><p><span style="font-size: 1rem;">

                   An important milestone of any autonomous vehicle is the capability of avoiding obstacles that lie in its path. In our case, due to the special shape and geometry of the rover, it was impractical or even not possible to use the lidar for such purpose, as the way it is currently mounted doesn’t allow a visibility on low obstacles and if put lower, most of its field of view would be blocked by the wheels, body or arm. For this reason we decided to go for a more simple construct consisting of the five ultrasonic sensors described above angled 45 degrees apart. This allows the rover to receive special information from all important directions. Knowing the geometry of the setup, it is then possible to start thinking about how we want to implement the obstacle avoidance. The choice was made to construct a Machine Learning (ML) genetic algorithm from scratch, relying on unsupervised training.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p>
	As the name suggests, this type of algorithm copies and simplifies natural selection by choosing the most fit individuals from a population of objects and keeping their traits for the next generation. To make the process as similar to real evolution as possible, those traits are also crossed between the fittest individuals, mimicking sexual reproduction, and mutations are introduced in the copying of the traits. The fitness is artificially created by the designer of the algorithm to fit its main purposes and often is the most difficult part to optimize, as it is difficult to predict how efficiently the ML will go in the wanted direction. This is generally called the fitness function of the genetic algorithm. The last important part of this implementation is the term unsupervised learning. Very often when talking about ML, the first examples are systems that rely on the programmer to feed examples to the algorithm for it to learn. For this reason it is called supervised learning and is found in programs that recognize handwritten letters, speech recognition and facial recognition, just to cite a few. This is very different from our case where the computer teaches itself to solve the task by trying to maximize the fitness of the population. By doing this, the programmers only have to create a good fitness function and then they can let the computer train by itself, without ever interacting.
In our case the idea is to create a simulation environment with a rover model, a goal and randomly generated obstacles. This rover should have 5 distance sensors pointed as described above and would know at any time how far the goal is from its current position. With these done we can then implement the genetic algorithm.
            </p><p><strong><br></strong></p>

                <p>
	Within the functioning of the AI, at every training session a population of rover models was created, each with a simple neural network as “genetic material”. This neural network takes as input the values of the five ultrasound sensors and the distance to the objective in a straight line and returns a value that indicates if the rover wants to make a rotation to the right or left and/or move forward. We chose to have a network with only one hidden layer of twelve nodes. This first population has it’s neural network filled randomly and is called the first generation. It’s size can be changed depending on the capabilities of your computer. This first generation is then let loose to measure the fitness of each member. In our case we went with a pretty simple fitness function that rewards the roves that get closest to the goal. There was also added a feature that reduces fitness everytime a rover rotates (see results and discussion). Once all rovers have destroyed themselves by running into walls or that the lifetime timer has passed, it is time for the selection, which is done by giving the highest probability of being picked to the member with highest fitness. Then comes the crossover and mutation, but instead of doing a strict crossover between individuals, we simply copy the neural network of the specimen that was chosen and mutate some of the connections in the neural network with a 10% probability. This means that all the members of our second generation are slightly mutated versions of the picked individual. And this process simply repeats itself, until we have theoretically arrived at a state where the rovers avoid all obstacles in the path to the goal.
            </p><p><strong><br></strong></p>

                 <p>
	The coding language was originally Python, but we switched to javascript for its web compatibility, as to make the whole modelling framework more interactive. In both cases the Processing framework was used to create the graphical interface for the training (Processing.py and p5.js) and Tensorflow.js was used to create the neural networks.
            </p><p><strong><br></strong></p>

                <p><strong>LiDAR Data Processing</strong></p><p><span style="font-size: 1rem;">

                   Originally, the LiDAR was set up according to the user manual provided by the manufacturer. This manual outlined the installation of several necessary drivers to use the LiDAR with Robot Operating System (ROS) software. This was configured originally on a RaspberryPi running raspbian OS, but after complications installing and operating several packages, the decision was made to switch to a disk image with Ubuntu 16 and ROS kinetic already installed. This configuration allowed for the real time viewing of lidar data through RVIZ, a visualisation tool used in conjunction with ROS. However, a key goal of the project was to have all data from the rover in a common web app, which would require access to the raw data. This could not be easily done through the RVIZ tool, and so it was necessary to subscribe a python script to the output data stream from the YDLidar X4. Once the data stream was accessed by the script, it was possible to use the matplotlib library on python to manually plot the data from the LiDAR. This script could then be implemented on the common web app and display the instantaneous map of the environment.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image8-1e">



    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBlidarrviz.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content6-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
                <strong> </strong></p><p><em>Figure 6: Screen captures of RVIZ and the LiDAR data (left: a, right: b)
</em></p>

                <p>
	Although a powerful mapping and localisation tool, LiDAR has some drawbacks. The most significant of these is that it can only image within a line-of-sight. This is demonstrated in figure 6, where the introduction of a wide obstacle in image b obscures a large portion of the environment. This drawback is an inherent flaw when dealing with lasers, and can limit the effectiveness of the LiDAR sensor in certain environments with many obstacles.

            </p><p><strong><br></strong></p>

                <p><strong>Arduino Control</strong></p><p><span style="font-size: 1rem;">
Most of the sensors were connected to an Arduino Mega board and their data was collected through a program written with the Arduino integrated environment (Arduino IDE), a software based on the coding language C++. A series of libraries were imported, which gave us the tools to collect data from the sensors and create a recognizable output. In the process of creating a script for all the sensors, the code for each module was first tested individually; if the code was successful, it was then organized and implemented in the complete program. Once all the sensors were included, the data was translated into a single String with specific headers, in order for the subsequent python scripts to retrieve the data through serial communication and storage for later use.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Data Collection and database population</strong></p><p><span style="font-size: 1rem;">
A USB cable was then used to establish a serial connection between Arduino and RaspberryPi. Such serial connection ensured that each of the sesnor’s readings was sent to the RaspberryPi in the form of a string. This data was then parsed and sorted out using a program written in Python 3. In the same script, the data was directly stored as a SQLite database, within two different tables, one for the sensors’ data and one for the GPS data. This distinction was made to facilitate the data visualization in the web app, for the sensor’s reading and the GPS tracking pages. The SQlite database was stored as a file on the RaspberryPi. This database then had to be converted into JSON to ensure an easier handling of data in the creation of the web app features, such as sensor data plots and GPS mapping. The graphs themselves were created using JavaScript and the Chart.js library, which acted as a means to create all the graphics. All the code responsible for the creation of the sensors’ data flask app can be found in the GitHub page
                    <a href="https://github.com/Jonastjep/CRAPy">https://github.com/Jonastjep/CRAPy</a>.
To ensure a timely recording of data with reliable timing the data receiving  code was run using Crontab, a software that allows to schedule software execution on linux distributions. The chosen time intervals for running the code was one hour, as we didn’t want to overflow the database with data, but yet wanted to collect relevant data that could indicate the fluctuations in environmental variables present in Mars-like habitats.


                </span></p><p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>


<section class="mbr-section article content9 cid-s2DGX1NZgS" id="content9-d">

     

    <div class="container">
        <div class="inner-container" style="width: 100%;">
            <hr class="line" style="width: 25%;">
            <div class="section-text align-center mbr-fonts-style display-5">
                    Results and Discussion</div>
            <hr class="line" style="width: 25%;">
        </div>
        </div>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content7-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>

                <p><strong>Obstacle Avoidance</strong></p><p><span style="font-size: 1rem;">
The creation of the framework was a success, as seen from Figures 8 and 9. The modeling of the rover is not yet perfect, as it is not a perfect square seen from above, but for the moment this is sufficient as this is only a first version of the simulation framework. More importantly the five ultrasound sensors are featured and can be seen working in Figure Z. At the moment the distance detectors only return a boolean value, meaning we don’t actually get a distance value from them, but changing this is planned, as this should give  more control to the simulated rover in tight spaces. The random generation of terrain was also a success even though it sometimes gives rise to goals impossible to reach.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image9-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBobstacleavoidance.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

<section class="mbr-section article content1 cid-s3o677EF4P" id="content8-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 8 and 9: Environment for AI training with rover
</em></p>
                <p>
The implementation of the genetic algorithm was also a partial success, as there definitely was some learning going on. Figure 10 shows a state of the learning process with most rovers still alive. In the good cases, the rovers would start picking up the ability of turning when encountering obstacles, but never to the point of autonomy and very often many rovers would simply start turning in circles indefinitely. This is the reason for creating a timer, as otherwise the training would rarely go more than a few generations before getting stuck in an endless loop. The model was also never trained for more than a few hundred generations of a hundred rovers and as such we don’t have a final answer on the capabilities of the algorithm to learn obstacle avoidance, there is high hopes that just with a few tweaks of the details we would obtain much better results. The most important of these is the fitness function, as for the moment the parameters were chosen arbitrarily. Because of the undesirable infinite loop behaviour of some rovers we implemented a negative fitness subtraction from the overall score. This had as an effect to reduce the jittering of the rovers and stop the cycling behaviour, as it would be un-favored in selection. This was a moderate success, as we saw less of the undesirable behaviour, but it didn’t work as well as planned and introduced other issues. One possibility for that is that the values subtracted to the overall fitness score were too large in comparison with the gains of going towards the goal and as such we mostly taught the rovers to go in a straight line. We however managed to see on multiple occasions very encouraging behaviour, with rovers learning to circle around obstacles.

               <p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image10-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBmultiplerovertraining.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content9-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 10: AI training with multiple rovers
</em></p>
                <p>
Because the code is written in javascript it is possible for anyone to run it on their browser by going to <a href="https://jonastjep.github.io/TinyProjects/">https://jonastjep.github.io/TinyProjects/</a> . There there is the choice of watching the AI learn or try to control the rover by yourself using the arrow keys to reach the goal.

               <p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Web App</strong></p><p><span style="font-size: 1rem;">
Seeing that the autonomous side of the project was not fully successful, a lot of effort was put into the control of the rover through the means of a web application served from the raspberry pi. We managed to create a nice graphical interface with multiple pages serving different control options or data gathered from missions. These different pages included the rover control page, basing itself on the SGVHAK software (Roger-random, 2018) as seen in Figures 11 and 12. The left hand figure shows the different control options and the right hand is the control virtual joystick for the wheel controls.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image12-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBwebapp.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content11-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 11 and 12: web app home screen and digital joystick
</em></p>
                <p>
Other implemented features include the robotic arm control, seen on figure 13 which allow to move the arm in all of its degrees of freedom, the interactive plots of the collected data on figure 14, a video stream from the wide angle IR camera with tilt and pan control seen in figure T as well as a live GPS map of the location of the rover with it’s full path. It is also able to query data.
               <p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

    <section class="cid-s3o5R9nIvc" id="image13-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBrobdata.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content12-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 13 and 14: Robotic arm controls and sensor data page
</em></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image14-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBarmmap.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content13-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 15 and 16: Rover video livestream and GPS sensor data map
</em></p>
                <p>
Unfortunately due to time constraints we were not able to implement the two last features, namely the thermal camera video stream and live point cloud from the LiDAR. These were only implemented using another system called websockets to feed it from the raspberry pi and as such were somewhat incompatible with Flask, but Flask offers the same capabilities and as such it is probably not too difficult to implement in the future. Figure R shows the live stream thermal camera with two fingers in front. There is still some work to be done for the calibration as a lot of noise is present. Figure 18 shows the LiDAR stream showing walls and objects inside a room. In this case also, there is visibly more noise than the RViz images, meaning there is probably some optimisation to be done.
               <p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image15-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBthermlid.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content14-1f">



    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 17 and 18: thermal camera and serial communication LiDAR through matplotlib
</em></p>

                <p><strong>LiDAR operation</strong></p><p><span style="font-size: 1rem;">
Several ROS libraries were investigated for their simultaneous localisation and mapping (SLAM) features, however most proved either too complicated to be integrated with our hardware, or required inertial measurements (IMU). The latter, although possible if the movement of the wheels was accounted for and recorded, was not available at the time of testing, and therefore prohibited the use of otherwise useful libraries. It was then decided that in order to properly integrate the LiDAR sensor data into the web app, the raw data should be extracted instead of attempting to use the RVIZ software. To achieve this, several python scripts were used in conjunction with publicly accessible libraries to extract serial data into a CSV file, which is much easier to use and provides better cross-compatibility over programming languages.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p>
Ultimately, the shift from ROS and RVIZ to a python script using matplotlib to plot the LiDAR data resulted in several changes. Primarily, and most noticeably, the latter came with significantly more noise (figure 18). Although unconfirmed, it is likely that the ROS software for which the lidar was designed to be used already has some form of cleanup when presented with raw data, and that the data presented on screen was in fact already processed to some degree. Furthermore, software like RVIZ and ROS are designed to be used with localisation and pose optimisation algorithms, and the drivers that were used to initially operate the LiDAR may have some form of pose optimisation already active. Regardless, when the raw data was used to build a map of the environment, there was significantly more noise, especially in regions that were known to be empty, and thus should have shown no artefacts on the constructed image. However, despite the noise, the manually generated maps were still successful at creating estimations of the environment, and objects within the plane were easily identifiable.
               <p><span style="font-size: 1rem;"><br></span></p>

                 <p><strong>Sensors</strong></p><p><span style="font-size: 1rem;">
The implementation of several types of sensors is an elemental step towards the optimization of the rover’s features. These modules entail a significant advance in the rover’s functionality within its environment, as it not only allows for interaction with the surroundings, but also provides a complete picture of the area being explored. During development, each sensor was tested separately, in which the modules for object location, gas detection, temperature and humidity, pressure and relative altitude, and soil moisture were all successful in providing the required data with high accuracy every two seconds. These components worked well together in the combined script, and addressed fundamental information that, once sent and analysed remotely, allows its users to recreate the conditions present in the setting.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p>
In addition, the GPS module was implemented in order to include a proof of concept for a GPS navigation system. Even though GPS is currently only used on earth, the importance of keeping the coordinates of the location of the rover at all times is key in mapping the explored environment. The results of this module were not all successful, as it proved challenging to connect the module to the required satellites and obtain the current information of latitude, longitude, and altitude. The GPS script for the previous Brover model was used to test the functionality of the device itself. Nonetheless, this script also presented problems and returned empty data. A possible explanation for this might be the sensitivity of the module, as the connections with the satellites are susceptible to any obstacle found in the way. The module was tested indoors, close to a window, but the construction of the building as well as any other physical interference might have prevented a successful connection to the three satellites needed. Moreover, the weather conditions along the testing days were not optimal, as there were mostly rainy days with a high density of clouds that could have blocked our module’s signal. Future improvements include achieving a stable connection with satellites, strong enough to defeat these types of restrictions.
               <p><span style="font-size: 1rem;"><br></span></p>

                <p>
Moreover, the radio frequency transceivers were an initial goal for the improvement of the rover’s communication with the users. Once the data is collected from the sensors, this module would allow to send the information wirelessly and monitor it remotely. This feature is key in increasing the range from which the rover can function, as it would not, and could potentially be used were there future plans of establishing pseudo-satellites along other planets need to be physically connected and could send, or receive, information from a distance. The implementation of this module was challenging, and the results were deemed to be inconclusive. The radio frequency antenna was able to send a simple string over to the receiver. However, the stability of the connection was not sufficient for our requirements; simply rotating or moving the antenna from place would cause the interaction to stop. Additionally, an acknowledgement receipt from the receiver is key to ensure proper communication and avoid any data loss. Throughout the testing days, only 7 acknowledgement receipts were sent, a ratio significantly small for the number of messages sent. A possible explanation for the constant disturbance of the signal might be the presence of other signals such as Wi-fi and Bluetooth connections in the testing room. In the same manner as with the GPS module, it is possible that in areas where the disturbances are kept at a minimum, the modules are able to maintain a stable connection and function properly. Thus, the script for both, the GPS and the radio frequency transceivers, was included in the final script of the Arduino sensors. Future improvements should aim at developing these connections to significantly improve the stability of the modules. Therefore, it was decided that the principal transmission between the Arduino microcontroller and the Raspberry pi, would be done through direct Serial communication. This would ensure no data is lost in the process and make it possible to recreate a complete picture of the environment the rover explores.
               <p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  </section>

  <section class="cid-s3o5R9nIvc" id="image16-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBarduino.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content15-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 19: Serial of the GPS data from the arduino
</em></p>
                <p>
The sensor implementation was mostly successful in achieving its main goal: the rover is now equipped with all the tools necessary to not only make sense of the environment that surrounds it but recreate it remotely for its study. Future studies should aim at increasing the rover’s capability and include more on-site modules to test and analyse its location.
               <p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Robotic arm and grabber</strong></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image17-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBrobotarm.png" width="1400" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content16-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 20: Robotic arm installed on the rover
</em></p>
                <p>
All parts were strong enough and there is no indication of them failing. The strength of the grip of the arm is limited, but the lifting and moving of a whole kitchen towel was undertaken successfully. This shows that the arm can fulfill its intended purpose. Currently, the arm suffers from continuous jerkiness when active. It is suspected that this issue comes from  the cables that provide power to the arm which cannot transmit the amperage required for smooth operation. As a next step, a thicker cable or multiple cables in parallel can be connected to the servos to get rid of the bottleneck. The compact design of the arm allows the motors to move it without issues. The potential problem of requiring rubber bands to aid the motors to lift the arm did not arise. The parts could be redesigned to be more efficient and elegant without the parts that allow the attachment of rubber bands. As the claws of the gripper are made of simple plastic, their grip is limited. To allow for greater payload and more reliable gripping, a grippy squishy foam could be attached to the grips. The current controls of the arm allow for all motion and leave the responsibility to avoid collision on the operator. An important future improvement to the arm would be to write security code to ensure that no commands are allowed which let the arm collide into the rover
               <p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>PETG Replacement Parts</strong></p><p><span style="font-size: 1rem;">
The PET-G replacement parts were successful and proved to be reliable. None of the replacement part failed catastrophically. In a few parts, the steering knuckles, cracks in the material were still observed. The source of this problem is suspected with problematic tolerances between the bearing surface and the bearing itself. With an increased bore diameter, the bearing could still have a snug fit within the knuckle without breaking the part. Additionally, the steering knuckle features one particularly thin section next to the bearing, which was consistently the area that cracked. This weak point could be easily modified in Fusion 360, in case the steering knuckles are ever replaced again.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Sensor Mast Design</strong></p><p><span style="font-size: 1rem;">
The sensor mast went through a few design iterations before arriving at the final design. Foreseeing a large number of parts to be printed towards the latter the project timeframe, the design of the mast was mostly completed in the first week and a half. This choice gave more time for physical assembly and allowed members to focus on other aspects of the project.  It did however create two issues when assembling and testing. The first was managing the large number of cables emerging from the head. Although the design had accounted for this, the change from jumper wire to 18 gauge wire resulted in a much larger total wire cross-section that had to be routed to the rover body. This meant that opening had to be cut into the supporting pipe for cables to pass, and that some slits had to be cut in the plastic to accommodate sensor wiring. The second issue was that the pillars supporting the top LiDAR plate could prohibit certain movements of the infrared and thermal cameras, as at certain angles the housing hit. For future designs, it would be advisable to design a C shaped brace that extends from the back of the base plate and curves over the top of the sensor housing to support the LiDAR. In this future design, the camera housing would not collide with the supports.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

                <p><strong>Sample Collection Box</strong></p><p><span style="font-size: 1rem;">
The sample collection box went through several iterations before the optimal design was decided upon, The first design was based on the concept of a sliding crank mechanism seen in Figure X. The servo motor would turn the arm thus moving the arm to push the lid open, then the opposite rotation of the motor would pull the arm back sliding the lid to cover the box once more. This design was rejected due to the need for several moving parts which may result in functional failure with prolonged use. Additionally, having this component would interfere with others due to the geometry of the box and the limited area provided to place it.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>

            </div>
        </div>
    </div>
</section>

  <section class="cid-s3o5R9nIvc" id="image18-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBbox1.png" width="900" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content17-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 21: Sliding box design
</em></p>
                <p>
The second version of the collection box was far more simple as it only required an axle with two rotating doors attached and spun 90º via a servo motor seen in Figure X. This model was printed and constructed onto the rover. While the model works in theory of holding and emptying contents, there is the possible limitation of soil samples being stuck between the axle thus not emptying the container as the doors rotate. Additionally, during the testing phase the collection box’s rotating doors were not able to turn consistently due to the motor to axle connection slipping out at times.Due to this reason, this model was deemed inefficient to complete the task required.
               <p><span style="font-size: 1rem;"><br></span></p>
       </div>
        </div>
    </div>
</section>

   <section class="cid-s3o5R9nIvc" id="image19-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBbox2.png" width="900" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content18-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 22: Rotating box design
</em></p>
                <p>
After testing of the second version of the collection box, there was low preparation time to ensure that a new design could be implemented. However, a third iteration was created in order to visualize a design that would improve upon previous versions. This design implemented both simplicity in regards to the opening and closing concept and also implemented some technicality in regards to the moving parts that caused it to function. A servo was placed to move an arm vertically which would rotate the door which covers the bottom of the collection box, turning it closed or turning open. This method ensured that all contents of the collection box are released without any residue or cleanup. The arm, servo and horn attachments were obtained from a CAD open source (Gupta, 2020) and adapted into the form similar to the second version of the collection box.
               <p><span style="font-size: 1rem;"><br></span></p>
       </div>
        </div>
    </div>
</section>
  <section class="cid-s3o5R9nIvc" id="image20-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBbox3.png" width="900" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content19-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 23: Trapdoor box design
</em></p>
                <p><strong>Solar Panel Mount and raiser</strong></p><p><span style="font-size: 1rem;">
A rack and pinion system was chosen to support the solar panel in its role, however due to time constraints of the study it was not physically implemented onto the rover. The system works by the turning effect of the piston on the rack, which causes the rack to move upwards for each turn against the teeth of the rack. These components would be placed underneath the solar panel causing it to tilt as the rack moves upward. A tilting effect was desired to make clear space for the arm to deposit into the collection box without spillage onto the solar panel. This in turn allows the solar panel to have at least two degrees of freedom to allow efficient charging of the rover’s battery.
                </span></p><p><span style="font-size: 1rem;"><br></span></p>
       </div>
        </div>
    </div>
</section>

   <section class="cid-s3o5R9nIvc" id="image21-1e">

    <figure class="mbr-figure container">
            <div class="image-block" style="width: 60%;">
                <img src="assets/images/JAMESWEBBbox4.png" width="900" alt="Mobirise" title="">
                <figcaption class="mbr-figure-caption mbr-figure-caption-over">
                    <div class="container pb-5 mbr-white align-center mbr-fonts-style display-1"></div>
                </figcaption>
            </div>
    </figure>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content20-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8"><p>
<strong> </strong></p><p><em>Figure 24: Rack and pinion design for linear actuator
</em></p>

                <p>
The main purpose of the sample collection box was to be an area where an x-ray fluorescence spectrometer can analyse the components in the box, however there was not an actual design for the spectrometer to house in. Designing the housing to incorporate the spectrometer is not difficult, instead the cost of such a device was the limiting factor which caused the component not being implemented in design .
               <p><span style="font-size: 1rem;"><br></span></p>

                 <p>
Mounting the solar panel was completed successfully and the raising of the solar panel was accounted for through CAD design and can be implemented in the future. However, there was no implementation for active use of the solar panel to charge the rover battery, instead only to set it up and tilt it on its hinge. In the future, this would be a prospective aim since the rover’s capabilities will increase dramatically as the rover is limited to the charging capacity of its battery. If there is no need to constantly recharge it, but instead it can be charged via sunlight in Mars-like conditions, it would already achieve a big step towards complete automation.
               <p><span style="font-size: 1rem;"><br></span></p>


       </div>
        </div>
    </div>
</section>

  <section class="mbr-section article content9 cid-s2DGMDPdzu" id="content21-9">



    <div class="container">
        <div class="inner-container" style="width: 100%;">
            <hr class="line" style="width: 25%;">
            <div class="section-text align-center mbr-fonts-style display-5">
                    Conclusion</div>
            <hr class="line" style="width: 25%;">
        </div>
        </div>
</section>

  <section class="mbr-section article content1 cid-s3o677EF4P" id="content21-1f">
    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8">

                <p>
The CRAP-E project succeeded in implementing many of the intended features. Multiple cameras, a LiDAR and five ultrasound sensors were installed and implemented to equip the rover with the tools to be moving autonomously. However, the genetic AI to pair with such tools was created, yet not successfully implemented due to time constraints. The web-app controlled to robotic arm was successfully added and operated. Moreover, the web app was also capable of displaying the data collected from the sensors under the form of interactive graphs. The tiltable solar panel remained in the stages of early cad design and was not successfully implemented. A design of the sample box was printed, but was not operational. In summary, many of the ambitious goals were reached, and some still remain to be achieved, yet it can be said that the CRAP-E rover was a successful improvement to its predecessor.

                </span></p><p><span style="font-size: 1rem;"><br></span></p>
       </div>
        </div>
    </div>
</section>


<section class="mbr-section article content9 cid-s3o8j9V4iN" id="content9-1m">
    
     

    <div class="container">
        <div class="inner-container" style="width: 100%;">
            <hr class="line" style="width: 25%;">
            <div class="section-text align-center mbr-fonts-style display-5">
                    References</div>
            <hr class="line" style="width: 25%;">
        </div>
        </div>
</section>

<section class="mbr-section article content1 cid-s3o8o5qvtI" id="content1-1n">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-7 col-md-8">
                <p><span style="font-size: 1rem;">Ada, L. (2021). Adafruit BMP280 Barometric Pressure + Temperature Sensor Breakout. Retrieved January 26, 2021, from https://learn.adafruit.com/adafruit-bmp280-barometric-pressure-plus-temperature-sensor-breakout</span><br></p>
                <p>Arduino. (2019). Complete Guide to Use Soil Moisture Sensor w/ Examples. Retrieved January 26, 2021, from https://create.arduino.cc/projecthub/electropeak/complete-guide-to-use-soil-moisture-sensor-w-examples-756b1f</p>
                <p>Arduino. (2019). How to Interface Arduino Mega with NEO-6M GPS Module. Retrieved January 26, 2021, from https://create.arduino.cc/projecthub/ruchir1674/how-to-interface-arduino-mega-with-neo-6m-gps-module-1b7283</p>
                <p>Gupta, A. (2020, July 26). Servo controlled Container. Retrieved from: https://grabcad.com/library/servo-controlled-container-1 Accessed: (January 25th, 2021)</p>
                <p>Lacroix, Simon, et al. "Autonomous rover navigation on unknown terrains: Functions and			 integration." The International Journal of Robotics Research 21.10-11 (2002): 917-942.</p>
                <p>How to mechatronics (2018). DIY Arduino Robot Arm with Smartphone Control. Retrieved from: https://www.youtube.com/watch?v=_B3gWd3A_SI&t=192s (January 25th, 2021: 11:52 CET)</p>
                <p>Jabbar, A. (2019). Ultrasonic Sensor HC-SR04 with Arduino Tutorial. Retrieved January 26, 2021, from https://create.arduino.cc/projecthub/abdularbi17/ultrasonic-sensor-hc-sr04-with-arduino-tutorial-327ff6</p>
                <p>Last Minute Engineer. (n.d.). How MQ2 Gas/Smoke Sensor Works? & Interface it with Arduino. Retrieved January 24, 2021, from https://lastminuteengineers.com/mq2-gas-senser-arduino-tutorial/</p>
                <p>Last Minute Engineers. (n.d.). Interface DHT11 Module With Arduino. Retrieved January 26, 2021, from https://lastminuteengineers.com/dht11-module-arduino-tutorial/ </p>
                <p>Last Minute Engineers. (2020, December 18). In-Depth: How nRF24L01 Wireless Module Works & Interface with Arduino. Retrieved January 26, 2021, from https://lastminuteengineers.com/nrf24l01-arduino-wireless-communication/</p>
                <p>Izgarevic, D. (2019, November 13). What Is Fusion 360? Retrieved from: https://all3dp.com/2/what-is-fusion-360-simply-explained/ Accessed: (January 25th, 2021)</p>
                <p>Vinyas, M., Athul, S. J., Harursampath, D., & Thoi, T. N. (2019). Experimental evaluation of the mechanical and thermal properties of 3D printed PLA and its composites. Materials Research Express, 6(11), 115301</p><p><strong><br></strong></p></div>
        </div>
    </div>
</section>

  <script src="assets/web/assets/jquery/jquery.min.js"></script>
  <script src="assets/popper/popper.min.js"></script>
  <script src="assets/bootstrap/js/bootstrap.min.js"></script>
  <script src="assets/tether/tether.min.js"></script>
  <script src="assets/smoothscroll/smooth-scroll.js"></script>
  <script src="assets/parallax/jarallax.min.js"></script>
  <script src="assets/dropdown/js/nav-dropdown.js"></script>
  <script src="assets/dropdown/js/navbar-dropdown.js"></script>
  <script src="assets/touchswipe/jquery.touch-swipe.min.js"></script>
  <script src="assets/theme/js/script.js"></script>
  
  
</body>
</html>